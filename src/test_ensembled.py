import tensorflow as tf
from tensorflow import keras
import numpy as np
import os
import glob
from PIL import Image, ImageOps
from transformers import TFCLIPVisionModel, CLIPProcessor
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm


# =========================================================
# 1. ê²½ë¡œ ì„¤ì • (ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •!)
# =========================================================
RESNET_MODEL_PATH = "best_model_EPOCH100.h5"
GENDET_MODEL_PATH = "gendet/"
TEST_DATA_DIR = "data/dataset/test"  # 0_real, 1_fake í´ë”ê°€ ìˆëŠ” ê³³

# =========================================================
# 2. ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì½”ë“œì™€ ë™ì¼)
# =========================================================
class TransformerBlock(keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):
        super(TransformerBlock, self).__init__(**kwargs)
        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential([
            keras.layers.Dense(ff_dim, activation="relu"),
            keras.layers.Dense(embed_dim),
        ])
        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = keras.layers.Dropout(rate)
        self.dropout2 = keras.layers.Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

print("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘... (ì„œë²„ ì—†ì´ ë¡œì»¬ì—ì„œ ì‹¤í–‰)")

# GPU ì„¤ì •
gpus = tf.config.list_physical_devices("GPU")
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except: pass

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
resnet_model = tf.keras.models.load_model(RESNET_MODEL_PATH)
custom_objects = {"TransformerBlock": TransformerBlock}

# ê²½ë¡œ ìˆ˜ì • ì£¼ì˜ (teacher/student/classifier ê°ê° ë¡œë“œ)
teacher_model = keras.models.load_model(os.path.join(GENDET_MODEL_PATH, "teacher.keras"), custom_objects=custom_objects, compile=False)
student_model = keras.models.load_model(os.path.join(GENDET_MODEL_PATH, "student.keras"), custom_objects=custom_objects, compile=False)
classifier_model = keras.models.load_model(os.path.join(GENDET_MODEL_PATH, "classifier.keras"), custom_objects=custom_objects, compile=False)

clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_vision_model = TFCLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# =========================================================
# 3. ì¶”ë¡  í•¨ìˆ˜ ì •ì˜
# =========================================================

def get_resnet_score(img_path):
    try:
        img = Image.open(img_path).convert('RGB')
        img = ImageOps.pad(img, (256, 256), color='black')
        
        img_array = tf.keras.utils.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        
        # ğŸ”´ [ìŠ¤ìœ„ì¹˜ ì‘ë™] ì—¬ê¸°ì„œ ì •ê·œí™” ì—¬ë¶€ ê²°ì •
        if USE_RESCALE:
            img_array = img_array / 255.0
        
        pred = resnet_model.predict(img_array, verbose=0)
        return float(pred[0][0])
    except:
        return 0.5 # ì—ëŸ¬ì‹œ ì¤‘ë¦½

def get_gendet_score(img_path):
    try:
        with open(img_path, "rb") as f:
            img_bytes = f.read()
        
        img_bytes_tf = tf.convert_to_tensor(img_bytes, dtype=tf.string)
        img = tf.image.decode_image(img_bytes_tf, channels=3, expand_animations=False)
        img = tf.image.resize(img, [224, 224])
        img = tf.cast(img, tf.float32) / 255.0
        img_uint8 = (img.numpy() * 255).astype(np.uint8)
        
        inputs = clip_processor(images=img_uint8, return_tensors="tf", padding=True)
        pixel_values = inputs["pixel_values"]
        
        features_numpy = clip_vision_model(pixel_values=pixel_values).pooler_output.numpy()
        
        z_t = teacher_model(features_numpy, training=False)
        z_s = student_model(features_numpy, training=False)
        abs_diff = tf.abs(z_t - z_s)
        score = float(classifier_model(abs_diff, training=False).numpy().reshape(-1)[0])
        return score
    except:
        return 0.5

# =========================================================
# 4. ì „ì²´ ë°ì´í„° í‰ê°€ ì‹¤í–‰
# =========================================================

real_paths = glob.glob(os.path.join(TEST_DATA_DIR, "real", "*.*"))
fake_paths = glob.glob(os.path.join(TEST_DATA_DIR, "fake", "*.*"))

print(f"\nğŸ“‚ Real Images: {len(real_paths)}ì¥")
print(f"ğŸ“‚ Fake Images: {len(fake_paths)}ì¥")

y_true = []
y_pred = []
y_scores = [] # ROC-AUCìš© (Fakeì¼ í™•ë¥ )

print(f"\nğŸš€ í‰ê°€ ì‹œì‘! (USE_RESCALE = {USE_RESCALE})")

# 1) Real í‰ê°€ (ì •ë‹µ 0)
print("Processing Real Images...")
for path in tqdm(real_paths):
    s_res = get_resnet_score(path)
    s_gen = get_gendet_score(path)
    
    # ì•™ìƒë¸”: í‰ê· (Average) ë°©ì‹ ì‚¬ìš©
    final_score = (s_res + s_gen) / 2.0
    
    y_true.append(0)
    y_scores.append(final_score)
    y_pred.append(1 if final_score > 0.5 else 0)

# 2) Fake í‰ê°€ (ì •ë‹µ 1)
print("Processing Fake Images...")
for path in tqdm(fake_paths):
    s_res = get_resnet_score(path)
    s_gen = get_gendet_score(path)
    
    final_score = (s_res + s_gen) / 2.0
    
    y_true.append(1)
    y_scores.append(final_score)
    y_pred.append(1 if final_score > 0.5 else 0)

# =========================================================
# 5. ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥
# =========================================================
print("\n" + "="*60)
print(f"ğŸ“Š [Evaluation Result] (Rescale: {USE_RESCALE})")
print("="*60)

acc = accuracy_score(y_true, y_pred)
auc = roc_auc_score(y_true, y_scores)

print(f"âœ… Accuracy : {acc:.4f}")
print(f"âœ… ROC-AUC  : {auc:.4f}")
print("\nğŸ“„ Detailed Report:")
print(classification_report(y_true, y_pred, target_names=['Real', 'Fake']))

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Pred Real', 'Pred Fake'], 
            yticklabels=['Actual Real', 'Actual Fake'])
plt.title(f'Confusion Matrix (Rescale={USE_RESCALE})')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show() # ë¡œì»¬ í™˜ê²½ì´ë©´ ì°½ì´ ëœ¨ê³ , ì„œë²„ë©´ ì—ëŸ¬ë‚  ìˆ˜ ìˆìŒ (ê·¸ë• plt.savefig ì‚¬ìš©)